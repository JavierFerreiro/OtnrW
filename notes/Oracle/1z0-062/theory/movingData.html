<h2>Moving Data</h2>
<p>
	Oracle provides two facilites to move data: Data Pump and SQL*Loader. Data Pump can transfer data between Oracle databases (across versions and platforms) whereas SQL*Loader can read data sets generated by non-Oracle systems.
</p>
<ul>
	<li>Describe ways to Move Data<br>
		Four differents ways: SQL*Loader, Data Pump, external tables or database links.
		<ul>
			<li>Create and Use Directory Objects.<br>
				Oracle directory objects allow sessions against the database to read and write operating system files. Some Oracle utilities (such as Data Pump) require directories. Oracle directories provide a layer of abstraction between the user and the O.S. As DBA create a directory object within the database, wich points to a physical path on the file system. Permissions on these Oracle directories can then be granted to individual database users. At O.S. level the Oracle user will need permissions against the O.S. directories to wich the Oracle directories refer.<br>
				Note: Directories are always owned by user SYS, but any user to whom you have granted the CREATE ANY DIRECTORY privilege can create them.<br>
				Oracle directories can be created with CREATE DIRECTORY command. To see information about directories use DBA_DIRECTORIES. Each directory has a name, an owner and the physical path to wich it refer. Note that Oracle does not verify whether the path exists when you create the directory (the errors success when is used). How to:<br>
				<code>
					SQL&gt; connect / as sysdba<br>
					SQL&gt; grant create any directory to scott;<br>
					SQL&gt; connect scott/tiger <br>
					SQL&gt; create directory scott_dir as '/home/scott';<br>
					SQL&gt; grant read on directory scott_dir to public;<br>
					SQL&gt; grant write on directory scott_dir to hr;<br>
					SQL&gt; drop directory scott_dir;<br>
					ERROR at line1:<br>
					ORA-01031: Insufficient privileges<br>
					SQL&gt; connect / as sydba;<br>
					SQL&gt; drop directory scott_dir;
				</code></br>
				In this code you can see that the directory is owned by SYS: directories are not schema objects, this is why SCOTT cannot drop the directory, even though he createrd it. To drop a directory requires another privilege: DROP ANY DIRECTORY.
			</li>
			<li>Use SQL*Loader to Load Data from a Non-Oracle Database<br>
				In many cases you will be faced with a need to do a bulk upload of datasets generated from some third-party system. This is the purpose of SQL*Loader.<br>
				<ul>
					<li>Using SQL*Loader<br>
						Architecturally, SQL*Loader is a user process like any other: it connects to the DB via a server process, issuing a connect string that identifies a database listener or using ORACLE_SID (in the same machine). To insert rows, it can use one of two techniques: conventional or direct path. A conventional insert uses absolutely ordinary INSERT statements. The SQL*Loader user process constructs an INSERT with bind variables in the VALUES clause and then reads the source datafile to execute the INSERT once for each row to be inserted. This method uses the database buffer cache and generates undo and redo data: these are INSERT statements like any others, and normal commit processing makes them permanent.<br>
						The direct path load bypasses the database buffer cache. SQL*Loader reads the source datafile and sends its contents to the server process. The server process then assembles blocks of table data in its PGA and writes them directly to the datafiles. The write is above the high water mark  of the table and is known as a data save. The high water mark is a marker in the table segment above wich no data has ever been written: the space above the high water mark is a space allocated to the table that has not yet been used. Once the load is complete, SQL*Loader shifts water mark up to include the newly written blocks, and the rows withing them are then immediately visible to other users. This is the equivalent of a COMMIT. No undo is generated, and if you wish, you can switch off the generation of redo as well. For these reasons, direct path loading is extremely fast and furthermore it should not impact on your end users because interraction with the SGA is kept to a minimum. The drawbacks are: referential integrity constraints must be dropped or disabled for the duration of the operation, insert triggers do not fire, the table will be locked against DML from other sessions and it's not possible to use direct path for clustered tables. These limitations are a result of the lack of interaction with the SGA while the load is in progress.<br>
						Note: only UNIQUE, PRIMARY KEY, and NOT NULL constraints are enforced during a direct path load. INSERT triggers do not fire, and the tables is locked for DML.<br>
						In SQL*Loader, the input datafiles are the source data that it will upload into the database. The controlfile is a text file with directives telling SQL*Loader how to interpret the contents of the input files, and what to do with the rows it extracts from them. Log files summarize the success or failure of the job, with detail of any errors. Rows extracted from the input files may be rejected by SQL*Loader for differents reasons, this rows are written t the bad file. If row are correct but rejected because they did not match some record-selection criteria, they are written out to a reject file.<br>
						Note: All files related to SQL*Loader are client-side files, not server-side files.<br>
						The controlfile is a text file instructiong SQL*Loader on how to process the input datafiles. It's possible include the actual data to be loaded on the controlfile, but you would not normally do this. Example of data and control file to read the data:<br>
						
						data:<br>
						
						table:<br>
						<code>
							DEPTNO(not null,numner(2)),DNAME(varchar2(14)),LOC(varchar2(13)).
					    </code><br>
						data:<br>
						<code>
							60,CONSULTING,TORONTO.<br>
							70,HR,OXFORD.<br>
							80,EDUCATION,<br>
						</code><br>
						controlfile(depts.ctl): <br>
						<code>
							load data<br>
							infile 'dept.dat'<br>
							badfile 'depts.bad'<br>
							discardfile 'depts.dsc'<br>
							append<br>
							into table dept<br>
							fields terminated by ','<br>
							trailing nullcols<br>
							&nbsp;(deptno integer external(2),dname,loc)
						</code><br>
						To perform the load run:<code>sqlldr userid=scott/tiger@orcl control=depts.ctl</code><br>
						Adding DIRECT=TRUE argument would instruct SQL*Loader to use diret path rather than a conventional insert (wich is the default).
					</li>
					<li>SQL* Loader Express Mode<br>
						There is a very simple way to use SQL*Loader that requires no controlfile. This is the express mode: <code>sqlldr scott/tiger@orclz table=dept</code><br>
						To use express mode:must exsit a datafile with the same name as the table to be loaded with suffixed .DAT, the columns must be a scalar data type: character, number or date, the fields in the file must be comma delimited and not exlosed in quotes and the input rows must have values for every column of the table.<br>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>Use External Tables to Move Data via Platform-Independent Files<br>
		An external table is visible to SELECT statements as any other table, but you cannot perform DML against it. This is because it does not exist as a segment in the database, it exists only as a data dictionary construct, pointing toward one or more O.S. files. The O.S. files are located through Oracle directory objects.<br>
		A common use of external tables is to avoid needing to use SQL*Loader to read into the database saving time in ETL (extract-transform-load) cycle typically used to update a DSS (decision suport system) from a feeder system.<br>
		To create an external table, use the CREATE TABLE command with the keywords ORGANIZATION EXTERNAL. Example:<br>
		<code>
			create table nwe_dept (<br>
			&nbsp;&nbsp;&nbsp;dept number(2),<br>
			&nbsp;&nbsp;&nbsp;dname varchar2(14),<br>
			&nbsp;&nbsp;&nbsp;loc varchar2(13)<br>
			&nbsp;&nbsp;&nbsp;)<br>
			organization external (<br>
			 type oracle_loader <br>
			 default directory ext_dir <br>
			 access parameters(<br>
			 &nbsp;&nbsp;&nbsp;records delimited by newline<br>
			 &nbsp;&nbsp;&nbsp;badfile 'depts.bad'<br>
			 &nbsp;&nbsp;&nbsp;discardfile 'depts.dsc'<br>
			 &nbsp;&nbsp;&nbsp;logfile 'depts.log'<br>
			 &nbsp;&nbsp;&nbsp;fields terminated by ','<br>
			 missing field values are null<br>
			 )<br>
			 location ('depts.dat'))
		</code><br>
		External tables can be queried in exactly the same way as internal tables. Any SQL involving a SELECT will function against an external tables (could be used in joins, views and subqueries). They cannot have indexes, constraints or triggers.
	</li>
	<li>Explain the General Architecture of Oracle Data Pump<br>
		Data Pump is a server-sice utility. You initiate Data Pump jobs from a user process, but all the work is done by server processes. This improves performance over export/import utilities, because running on the server have direct access to the datafiles and the SGA; they do not have to go via a session. Also, it is possible to launch a Data Pump job and then detach from it, leaving it running in the background. You can reconnect to the job to monitor it's progress at any time.<br>
		The process involved are: the user processes are expdp and impdp. These are used to launch, control and monitor data pump jobs. The expdp or impdp user process establishes a session against the database through a normal server process, either locally or via a listener. This sesion then issues commands to control and monitor Data Pump jobs. When a data pump job is launched, at least two processes are started: a data Pump Master (DMnn) and oner or more worker processes (DWnn). The master process controls the workers.<br>
		Two queues are created for each Data Pump job: a control queue and a status queue. The DMnn divides up the work to be done and places individual tasks that make up the job on the control queue. The qorker processes pick up these tasks and execute them- perhaps making use of parallel execution servers. This queue operates on a deliver-exactly-once model: messages are enqueued by the DMnn and dequeued by ther worker that picks them up. The status queue is for monitoring purposes: the DMnn places messages on it describing the state of the job. This queue operates on a publish-and-subscribe model: any session (with appropiate privileges) can query the queue to monitor the job's progress.<br>
		The files generated by data Pump come in three forms: SQL files, dump files, and log files. SQL files are DDL statements describing the objects included in the job. You can choose to generate them (without any data) as an easy way of getting this information out of the DB, perhaps for documentation purposes or as a set os scripts to re-create the DB. Dump files contain the exported data. This is formatted in a fashion resembling XML tags. The log files describe the historyof the job run.<br>
		Note: Data Pump can be invoked by a client, but it runs on the server. All the files are server-side files, with nothing on the client side.<br>
		Finally, theres is the control table. This is created for you by the DMnn when you launch a job, and is used both to record the job's progress and to describe it. It is included in the dump file as the final item of the job.<br>
		Data Pump has two methods for loading and unloading data: the direct path and the external table path. The direct path bypasses the database buffer cache. For a direct path export, Data Pump reads the datafile blocks directly from disk, extracts and formats the content and writes it out as a dump file. For a direct path import, Data Pump reads the dump file, uses its content to assemble blocks of table data, and writes them directly to the datafiles. The write is above the "high water mark" of the table, with the same benefits as those described for SQL*Loader direct load.<br>
		The external table path uses the database buffer cache. Even though Data Pump is manipulating files that are external to the DB, it uses the database buffer cache as though it were reading and writing and internal table. For an export, Data Pump reads blocks from the datafiles into the cache through a normal SELECT process. From there, it formats the data for output to a dump file. During an import, Data Pump constructs standard insert from the content of the dump file and executes them by reading blocks from the datafiles into the cache, where the insert is carried out in the normal fashion. For DB, external table Data Pump jobs look like absolutely ordinary (though perhaps rather large) SELECT or INSERT operations. Both undo and redo are generated, as they would be for any normal DML statement.<br>
		The DBA has no control of the decission to use Data Pump with direct path or external table path. Data Pump itself makes the decision based on the complexity of the objects. Only simple structures, such as heap tables without active triggers, can be processed through the direct path; more complex objects such as clustered tables force Data Pump to use the external table path because it requires interaction with the SGA on order to resolve the complexities.<br>
		Note: The external table path insert uses a regular commit, like any other DML statement. A direct path insert does not use a commit; it simply shifts the high water mark of the table to include the newly written blocks. Data Pump files generated by either path are identical.
	</li>
	<li>Use Data Pump Export and Import to Move Data Between Oracle Databases<br>
		<p>
			The files generated by Data Pump are in propietary format. You must use Data Pump tools to read it.
		</p>
		<ul>
			<li>Capabilities<br>
				<ul>
					<li>
						Fine-grained object and data selection facilities mean that Data Pump can export either the complete DB or any part of it: table definitions(with or without rows), PL/SQL objects, views, sequences, or any other object type.
					</li>
					<li>
						If exporting a table it's possible to apply WHERE clause to restrict the rows exported (altohough this may make direct path impossible) or to instruct Data Pump to export a random sample of the table expressed as a percentage.
					</li>
					<li>
						Parallel processiong can speed up Data Pump operations. Parallelism can come at two levels: the number of Data Pump worker processes, and the number of parallel execution servers each worker process uses.
					</li>
					<li>
						An estimate facility can calculate the space needed for a data pump export, without actually running the job.
					</li>
					<li>
						The Network Mode allows transfer of a Data Pump dataset from one DB to another without ever staging it on disk. This is implemented by a Data Pump export job on the source db writing the data over a database link to the target db, where a Data Pump import job reads the data from the database link and inserts it.
					</li>
					<li>
						Remapping facilities that objects can be renamed or transferred from one schema to another and (in the case of data objects) moved from one tablespace to another as they are imported.
					</li>
					<li>
						When data is being exported, the output files can be compressed and encrypted.
					</li>
				</ul>
			</li>
			<li>Using Data Pump with the Command-Line utilities<br>
				The executables expdp and impdp are installed into $ORACLE_HOME/bin directory. Following are several examples of using them. Example: 
				<code> 
					expdp system/manager@orc112g full=y parallel=2 dumpfile=datadir1:full1_%U.dmp,datadir2:full2_%U.dmp filesize=2g compression=all/
				</code><br>
				This command will connect to db as user SYSTEM and launch a full Data Pump export, using two worker processes working in parallel. Each worker will generate its own set of dump files, uniquely named according to the %U template, wich generates unique strings of eight characters. Each worker will break up its output into files of 2GB (perhaps because of underlying file system restrictions) of compressed data.<br>
				The corresponding import job:<code>impdb system/manager@dev12g full0y directory=data_dir parallel=2 dumpfile=fulll1_%U.dmp,full2_%U.dmp</code></br>
				This command makes a selective export of the PL/SQL objects belonging to two schemas:<code>expdp system/manager schemas=hr,oe directory=code_archive dumpfile=hr_oe_code.dmp include=function, include=package, include=procedure, include=type</code><br>
				Finally this example export that was in the HR schema, and import it into the DEV schema:<code>impdp system/manager directory=usr_data dumpfile=usr_dat.dmp schema=hr remap_schema=hr:dev</code><br>
			</li>
			<li>Tablespace Export and Import<br>
				A variation on Data Pump export/import is the tablespace transport capability. This is a facility whereby entire tablespaces and their contents can be copied from one database to another. This is the routine:<br>
				<ol>
					<li>Make the source tablespace(s) read-only</li>
					<li>
						Use Data Pump to export the metadata describing the tablespace(s) and the contents
					</li>
					<li>Copy the datafile(s) and Data Pump export file to the destination system.
					</li>
					<li>Use Data Pump to import the metadata.</li>
					<li>Make the tablespace(s) read-write on both the source and destionation
					</li>
				</ol>
				An additional step that may be required when transporting tablespaces from one platform to another is to conver the endian format of the data.  A big-endian (such Solaris on SPARC chips) stores multibyte value as a 16-bit integer with the most significat byte first. A little-endian platform (such as Windows on Intel chips) stores the least significant byte first. To transport tablespaces across platforms with a different endia format requires converting the datafiles: you do this with the RMAN command CONVERT.<br>
				To determine the platform on wich a database is running, query the column PLATFORM_NAME in V$DATABASE. Then to see the list of currently supported platforms and their endiannes, query V$TRANSPORTABLE_PLATFORM<br>
				The restrictions for transportable tablespaces are:
				<ul>
					<li>
						The tablespaces(s) should be self-contained. This means that the objects within the tablespace(s) must be complete, not dependent on any other objects. For instance, if tables are in one tablespace and indexes on the tables in another, both tablespaces must be included in the set to be transported.
					</li>
					<li>
						The destination database must use the same (or compatible) character set as the source database.
					</li>
					<li>
						The schemas that own the objects in the tablespace must exist in the destination database, or the operation will fail.
					</li>
					<li>
						Any objects in the destination dabatase with the same owner and object name as objects in the transportable tablespace set will not be lost: they will be ignored during the import.
					</li>
					<li>
						A tablespace of the same name must not already exist.
					</li>
				</ul>
				example:<br>
				<code>
					SQL&gt; execute dbms_tts.transport_set_check('TS1');<br>
					SQL&gt; alter tablespace ts1 read only;<br>
					SQL&gt; host<br>
					[oracle@server ~]$ expdo system/oracle transport_tablespaces=ts1 dumpfile=ts1.dmp directory=dp_out<br>
				</code>
				If you need to convert of endiannes, connect to the source db with RMAN and run:<br>
				<code>
					convert datafile '/u02/oradata/ts1.dbf' to platform 'Solaris[tm] OE (64-bit)' format '/to_solaris/ts1.dbf'
				</code><br>
				This command will write out a copy of the file, with the endiannes changed. Alternatively, copy the unchandged file to the destination database, connect with RMAN, and run a command such as this:<br>
				<code>
					convert datafile '/from_linux/ts1.dbf' from platform 'Linux IA (64-bit)' format '/u02/oradata/ts1.dbf';
				</code><br>
				This command will read the nominated datafile, and convert it from the named platform format to a new file in the format that is required for the destination DB.<br>
				To import the tablespace(s) on the destination system, use a command such:<br>
				<code>
					[oracle@server ~]$ impdp system/oracle dumpfile=ts1.dmp directory=dp_in transport_datafiles=/u02/oradata/ts1.dbf<br>
				</code>
			</li>
		</ul>
	</li>
</ul>